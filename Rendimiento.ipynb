{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "//Adnan Bouaouda Arafa, Sept. 2021, UMA\r\n",
    "import org.apache.flink.api.scala._\r\n",
    "import org.apache.flink.api.scala.utils._\r\n",
    "import org.apache.flink.util.Collector\r\n",
    "import javax.script._\r\n",
    "import scala.annotation.tailrec\r\n",
    "import scalaz._\r\n",
    "import Scalaz._\r\n",
    "/**\r\n",
    " * Implements the \"eLTL\" logic\r\n",
    " */\r\n",
    "\r\n",
    "type T // use \"override type\" so specify the type in the user module\r\n",
    "def memoizeFnc[K, V](f: K => V): K => V = {\r\n",
    "val cache = collection.mutable.Map.empty[K, V]\r\n",
    "    k =>\r\n",
    "        cache.getOrElse(k, {\r\n",
    "        cache update(k, f(k))\r\n",
    "        cache(k)\r\n",
    "        })\r\n",
    "}\r\n",
    "\r\n",
    "def apply[T](b: DataSet[(Long, T)], cond: T => Boolean): DataSet[(Long, Boolean)] \r\n",
    "        =  b.map{ e => (e._1, cond(e._2))}\r\n",
    "val mApply = Memo.immutableHashMapMemo{apply _ tupled}\r\n",
    "\r\n",
    "def parse[T](b: DataSet[(Long, T)], cond: T => Boolean): List[Long]\r\n",
    "    = { lazy val lazy_b = apply(b, cond).filter{_._2}.map { pair => pair._1 }.collect.toList\r\n",
    "        lazy_b\r\n",
    "    }\r\n",
    "            \r\n",
    "def intervalPQ(lp: List[Long], lq: List[Long],  ti: Long = 0, tf: Long = Long.MaxValue): List[(Long, Long)] = {\r\n",
    "try{    \r\n",
    "    @tailrec def tIntervslPQ(Lp: List[Long], Lq: List[Long], acc: List[(Long, Long)] = List.empty[(Long, Long)]):List[(Long, Long)] = {\r\n",
    "        val  Lqq = Lq.dropWhile(_ <= Lp.head)\r\n",
    "        val  Lpp = Lp.dropWhile(_ <= Lqq.head)\r\n",
    "        (Lpp, Lqq) match {\r\n",
    "            case (List(), _) => acc ++ List((Lp.head, Lqq.head))\r\n",
    "            case (_, List()) => acc ++ List((Lp.head, Lqq.head))\r\n",
    "            case (_, _)   => tIntervslPQ(Lpp, Lqq.tail, acc ++ List((Lp.head, Lqq.head)))\r\n",
    "        }\r\n",
    "}\r\n",
    "    val  lpp = lp.sortWith(_ < _) filter (e => e >= ti  && e<= lq.max)\r\n",
    "    val  lqq = lq.sortWith(_ < _) filter (e => e >= ti  && e<= tf)\r\n",
    "    tIntervslPQ(lpp, lqq)\r\n",
    "}catch{case e: Exception => List.empty[(Long, Long)]}\r\n",
    "} \r\n",
    "val mIntervalPQ = Memo.immutableHashMapMemo{intervalPQ _ tupled}\r\n",
    "def intervalP(lp: List[Long],  ti: Long = 0, tf: Long = Long.MaxValue): List[(Long, Long)]={\r\n",
    "    val lpp = lp.sortWith(_ < _) filter (e => e >= ti  && e<= tf)\r\n",
    "    lpp zip lpp}\r\n",
    "val mIntervalP  = Memo.immutableHashMapMemo{intervalP _ tupled}\r\n",
    "def intervals[T](b: DataSet[(Long, T)], p: T => Boolean, q: T => Boolean = null, ti: Long = 0, tf: Long = Long.MaxValue)\r\n",
    "=  q match{\r\n",
    "        case null => mIntervalP(parse(b,p), ti, tf)\r\n",
    "        case _    => mIntervalPQ(parse(b,p), parse(b,q), ti, tf)\r\n",
    "    }\r\n",
    "def bPQ[T](b: DataSet[(Long, T)], i: List[(Long, Long)])\r\n",
    "= i.map(e => b.filter(x => (x._1 >= e._1) && (x._1 <= e._2)))\r\n",
    "def Phi[T](b: DataSet[(Long, T)], ti: Long = 0, tf: Long = Long.MaxValue): Boolean = ???\r\n",
    "def True[T](b: DataSet[(Long, T)], ti: Long = 0, tf: Long = Long.MaxValue): Boolean = true\r\n",
    "def PhiAll[T](p: T => Boolean)(b: DataSet[(Long, T)], ti: Long, tf: Long): Boolean = {\r\n",
    "        val bb = b.filter(e => (e._1 >= ti) && (e._1 <= tf))\r\n",
    "        bb.map(e => p(e._2)).collect.forall(_ == true)\r\n",
    "}\r\n",
    "def PhiK[T](K: Long)(p: T => Boolean, q: T => Boolean = null)\r\n",
    "    = {(b: DataSet[(Long, T)], ti: Long, tf: Long) \r\n",
    "    => intervals(b, p, q, ti, tf).map(e => (e._2 - e._1 >= K)).\r\n",
    "    reduceOption(_ || _).getOrElse(false)}\r\n",
    "def Neg[T](F: (DataSet[(Long, T)], Long, Long) => Boolean)\r\n",
    "    ={(b: DataSet[(Long, T)], ti: Long, tf: Long) => !F(b, ti, tf)}\r\n",
    "def Or[T](F1: (DataSet[(Long, T)], Long, Long) => Boolean, \r\n",
    "        F2: (DataSet[(Long, T)], Long, Long) => Boolean)\r\n",
    "    = {(b: DataSet[(Long, T)], ti: Long, tf: Long) \r\n",
    "    => F1(b, ti, tf) || F2(b, ti, tf)}  \r\n",
    "def U[T](p: T => Boolean, q: T => Boolean = null)\r\n",
    "    (F1: (DataSet[(Long, T)], Long, Long) => Boolean,\r\n",
    "    F2: (DataSet[(Long, T)], Long, Long) => Boolean)\r\n",
    "    = {(b: DataSet[(Long, T)], ti: Long, tf: Long)\r\n",
    "    => intervals(b, p, q, ti, tf).map(e => F1(b, ti, e._1) && F2(b, e._1, e._2)).\r\n",
    "    reduceOption(_ || _).getOrElse(false)}                                 \r\n",
    "def E[T](p: T => Boolean, q: T => Boolean = null)\r\n",
    "    (F: (DataSet[(Long, T)], Long, Long) => Boolean)\r\n",
    "    = {(b: DataSet[(Long, T)], ti: Long, tf: Long) \r\n",
    "    =>  val I = intervals(b, p, q, ti, tf); I.map(e => F(b, e._1, e._2)).\r\n",
    "    reduceOption(_ || _).getOrElse(false)}\r\n",
    "def E2[T](p: T => Boolean, q: T => Boolean = null)\r\n",
    "        (F: (DataSet[(Long, T)], Long, Long) => Boolean)  = U(p, q)(True, F)\r\n",
    "def A[T](p: T => Boolean, q: T => Boolean = {(e: T) => true})\r\n",
    "        (F: (DataSet[(Long, T)], Long, Long) => Boolean)\r\n",
    "    = {(b: DataSet[(Long, T)], ti: Long, tf: Long)\r\n",
    "    =>  val I = intervals(b, p, q, ti, tf); I.map(e => F(b, e._1, e._2)).\r\n",
    "        reduceOption(_ && _).getOrElse(false)}\r\n",
    "def A2[T](p: T => Boolean, q: T => Boolean = null)\r\n",
    "        (F: (DataSet[(Long, T)], Long, Long) => Boolean)  = Neg(E(p, q)(Neg(F)))\r\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "cat ./csv/10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "type T = (Long, Int)\r\n",
    "type TT = Int\r\n",
    "val path = os.pwd / \"csv\"\r\n",
    "\r\n",
    "val _10      = benv.readCsvFile[T](path+\"/10.csv\", fieldDelimiter = \";\")\r\n",
    "\r\n",
    "def Start: (TT => Boolean) = (e: TT) => {e == 1}\r\n",
    "def Stop : (TT => Boolean) = (e: TT) => {e == 3}\r\n",
    "def Cond : (TT => Boolean) = (e: TT) => {e == 2}\r\n",
    "\r\n",
    "println(A(Start, Stop) (E(Cond) (True)) (_10, 0, Long.MaxValue))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
